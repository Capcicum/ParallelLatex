So what exactly is the goal of parallelizing work? This can be described in terms of latency and bandwidth. For a regular serial algorithm, when increasing performance, we will see a latency decrease. Latency in this case will be the time between giving our algorithm a stimulus until it returns a response. For algorithms discussed in this course stimulus will be data, and thereby the time it takes for an algorithm to be applied to some data. For most serial algorithms, to lower latency, the amount of data it is performed on will have to be decreased. Most current CPU's are still built to be optimized for latency, where a fast response is wanted for some calculation.\\
This differs greatly from what is the goal of parallelizing work. Here the goal is to maximize the throughput, thereby maximizing how must data we can perform our algorithm on in a given time span. This means that we are willing to sacrifice latency on any given calculation, if it improve the calculations performed over time. This relates to the before mentioned step and work complexity, where, if the step complexity is reduced, and the work complexity/latency is only slightly increased, the throughput of an algorithm can be increased.