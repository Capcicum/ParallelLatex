As threads are executed in parallel, there can be a need for synchronization between threads.
This can for instance be to ensure that all threads have moved their data to shared memory before starting to access it.
Synchronization of threads in a block can be done with the intrinsics call \textit{\_\_synchthreads()}, which acts as a barrier (as show in \autoref{lst:shared-mem-acc}).
The intrinsic must be used with caution, as all threads in the block wait for every thread to reach the synchronization point, and if one or more thread(s) have more work to do than the others, a lot of threads will remain idle until they done.

\subsection{Data Access}
Another synchronization mechanism that can be necessary to use is mutual exclusive data access.
This can be necessary if multiple threads for instance read and write to the same memory location.
To handle this issue, there are multiple \textit{atomic} functions that performs a read-modify-write atomic operation on data residing in either global or shared memory.
Examples are \textit{atomicAdd()} and \textit{atomicAnd()} \cite{cuda:programmingguide}.
Using atomic can give a performance penalty, as threads trying to access a memory location that is already being atomically modified must remain idle until the thread has finished its modification.

\subsection{Kernel and Stream Synchronization}
A different kind of synchronization that can be specified is between kernel calls.
Kernel launch is per definition asynchronous which means that it is possible to continue execution on the host after a kernel launch.
Waiting for a kernel to finish can explicitly be done with a function call to \textit{cudaDeviceSynchronize}, which waits for all the commands in the stream to finish (streams are described in \autoref{sec-pm-streams}).

\subsection{Memory Fences}
Modern GPU architectures have a relaxed memory model, which means that memory accesses are not guaranteed to be executed in the order they appear in the program.
This can in some rare cases be a problem, and memory fences are a way to deal with it.
Memory fences are a way to ensure that all memory accesses, performed prior to the fence, are executed before the fence.
In \cuda{} there are three levels of memory fences:
\begin{enumerate}
	\item For \textbf{Shared memory} use \textit{\_\_threadfence\_block()}
	\item For \textbf{Global memory} use \textit{\_\_threadfence()}
	\item For \textbf{Host memory} use \textit{\_\_threadfence\_system()}
\end{enumerate}
Each level also includes the previous level, so \textit{\_\_threadfence()} for instance is also a fence for shared memory \cite{cuda:programmingguide}.