So how does one approach parallelizing code or an algorithm, a systematic approach is called APOD which is an acronym for \texttt{Analyze}, \texttt{Parallelize}, \texttt{Optimize}, \texttt{Deploy}. APOD integrates software engineering principles, where an agile and iterative approach is taken to parallelize and optimize a program.\\
Each of these steps should be done individually, so given a problem, the first step is to \texttt{analyze}. The important part of the analyzation step is to first analyze to find out, what can be improved, where are the bottlenecks, how much can it be improved and is it worth the time to improve it.\
These are important, as they identify the critical sections in our program, with which we can then proceed to the next step \texttt{Parallelize}. in the parallelize step, the goal is to determine what approach can be taken to improve our problem. This could be choosing a library, picking an algorithm and/or programming the code. It is specifically important here that the right algorithm(s) for the task is chosen.\\
For the next step \texttt{Optimization}, the goal is as the name indicated to improve the program at hands. Primarily the focus here is on profile-driven optimization. It was described in \cref{sec-lat-vs-band} that with CUDA programming the goal is to improve throughput, which aligns with profile-driven optimization in that it is often possible to retrieve a measure on throughput, which can be used to compare whether optimization of the program actually improved throughput.\\
The final step \texttt{Deploy} is important because we want to test whenever improvement was beneficial to the "real" program. Does the solution achieved, solve our problem? did it help at all?\\ These questions are the reason why it is important that APOD is an iterative approach as if the answer was not to whether the problem was solved, a new round of APOD can be done, starting over with the \texttt{Analyse} step, again identifying bottlenecks. 